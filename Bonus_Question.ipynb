{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bonus_Question.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNcNrwULNNjQyg38LiPKn+S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afifrizkyandika11551100310/APRIL_DATA_SCIENTIST_ASSIGNMENT_M-Afif-Rizky-A/blob/main/Bonus_Question.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Scientist_APRIL_Assignment**\n",
        "**Name** : M Afif Rizky A <br>\n",
        "**Email** : afifrizky933@gmail.com / 23521034@std.stei.itb.ac.id "
      ],
      "metadata": {
        "id": "v2oN-dejqWHU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analytics Skills **Evaluation Requirement**:\n",
        "*\tQ1 & Q2 are mandatory, Q3 is bonus if attempted.\n",
        "*\tDo the analytics in cloud where possible (free trial versions).\n",
        "*\tOther methods are also acceptable.\n",
        "*\tDo include documentations"
      ],
      "metadata": {
        "id": "u3o1_WKsKP6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk, re, pprint, string\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "string.punctuation = string.punctuation +'“'+'”'+'-'+'’'+'‘'+'—'\n",
        "string.punctuation = string.punctuation.replace('.', '')\n",
        "file = open('text.txt', encoding = 'utf8').read()"
      ],
      "metadata": {
        "id": "rouI_RjaWs74"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVaWtv6HXmIu",
        "outputId": "4376d191-68bb-43bf-bad5-167a403c9fd2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8jlwee2YCvC",
        "outputId": "4c6f91c5-f001-46a5-e4e9-5bf2c73a62cd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_U-Gr_twrbw",
        "outputId": "0a391e74-96a5-409d-89ca-d991a4633cde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability of 'as': 0.00625\n",
            "Probability of 'a': 0.03125\n",
            "Probability of 'term': 0.009375\n",
            "Probability of 'data': 0.05625\n",
            "Probability of 'analytics': 0.03125\n",
            "Probability of 'predominantly': 0.003125\n",
            "Probability of 'refers': 0.003125\n",
            "Probability of 'to': 0.034375\n",
            "Probability of 'an': 0.003125\n",
            "Probability of 'assortment': 0.003125\n",
            "Probability of 'of': 0.03125\n",
            "Probability of 'applications': 0.003125\n",
            "Probability of 'from': 0.00625\n",
            "Probability of 'basic': 0.003125\n",
            "Probability of 'business': 0.0125\n",
            "Probability of 'intelligence': 0.003125\n",
            "Probability of 'bi': 0.00625\n",
            "Probability of 'reporting': 0.003125\n",
            "Probability of 'and': 0.028125\n",
            "Probability of 'online': 0.003125\n",
            "Probability of 'analytical': 0.003125\n",
            "Probability of 'processing': 0.003125\n",
            "Probability of 'olap': 0.003125\n",
            "Probability of 'various': 0.003125\n",
            "Probability of 'forms': 0.003125\n",
            "Probability of 'advanced': 0.00625\n",
            "Probability of 'in': 0.01875\n",
            "Probability of 'that': 0.015625\n",
            "Probability of 'sense': 0.003125\n",
            "Probability of 'it': 0.009375\n",
            "Probability of 's': 0.00625\n",
            "Probability of 'similar': 0.003125\n",
            "Probability of 'nature': 0.003125\n",
            "Probability of 'another': 0.003125\n",
            "Probability of 'umbrella': 0.003125\n",
            "Probability of 'for': 0.00625\n",
            "Probability of 'approaches': 0.003125\n",
            "Probability of 'analyzing': 0.003125\n",
            "Probability of 'with': 0.009375\n",
            "Probability of 'the': 0.034375\n",
            "Probability of 'difference': 0.003125\n",
            "Probability of 'latter': 0.003125\n",
            "Probability of 'is': 0.0125\n",
            "Probability of 'oriented': 0.003125\n",
            "Probability of 'uses': 0.00625\n",
            "Probability of 'while': 0.00625\n",
            "Probability of 'has': 0.00625\n",
            "Probability of 'broader': 0.003125\n",
            "Probability of 'focus': 0.003125\n",
            "Probability of 'expansive': 0.003125\n",
            "Probability of 'view': 0.00625\n",
            "Probability of 'isn': 0.003125\n",
            "Probability of 't': 0.003125\n",
            "Probability of 'universal': 0.003125\n",
            "Probability of 'though': 0.003125\n",
            "Probability of 'some': 0.003125\n",
            "Probability of 'cases': 0.003125\n",
            "Probability of 'people': 0.003125\n",
            "Probability of 'use': 0.003125\n",
            "Probability of 'specifically': 0.003125\n",
            "Probability of 'mean': 0.003125\n",
            "Probability of 'treating': 0.003125\n",
            "Probability of 'separate': 0.003125\n",
            "Probability of 'category': 0.003125\n",
            "Probability of 'initiatives': 0.003125\n",
            "Probability of 'can': 0.015625\n",
            "Probability of 'help': 0.003125\n",
            "Probability of 'businesses': 0.003125\n",
            "Probability of 'increase': 0.003125\n",
            "Probability of 'revenues': 0.003125\n",
            "Probability of 'improve': 0.003125\n",
            "Probability of 'operational': 0.003125\n",
            "Probability of 'efficiency': 0.003125\n",
            "Probability of 'optimize': 0.003125\n",
            "Probability of 'marketing': 0.003125\n",
            "Probability of 'campaigns': 0.003125\n",
            "Probability of 'customer': 0.003125\n",
            "Probability of 'service': 0.003125\n",
            "Probability of 'efforts': 0.003125\n",
            "Probability of 'respond': 0.003125\n",
            "Probability of 'more': 0.00625\n",
            "Probability of 'quickly': 0.003125\n",
            "Probability of 'emerging': 0.003125\n",
            "Probability of 'market': 0.003125\n",
            "Probability of 'trends': 0.003125\n",
            "Probability of 'gain': 0.003125\n",
            "Probability of 'competitive': 0.003125\n",
            "Probability of 'edge': 0.003125\n",
            "Probability of 'over': 0.003125\n",
            "Probability of 'rivals': 0.003125\n",
            "Probability of 'all': 0.003125\n",
            "Probability of 'ultimate': 0.003125\n",
            "Probability of 'goal': 0.003125\n",
            "Probability of 'boosting': 0.003125\n",
            "Probability of 'performance': 0.003125\n",
            "Probability of 'depending': 0.003125\n",
            "Probability of 'on': 0.00625\n",
            "Probability of 'particular': 0.003125\n",
            "Probability of 'application': 0.003125\n",
            "Probability of 'analyzed': 0.003125\n",
            "Probability of 'consist': 0.003125\n",
            "Probability of 'either': 0.003125\n",
            "Probability of 'historical': 0.003125\n",
            "Probability of 'records': 0.003125\n",
            "Probability of 'or': 0.0125\n",
            "Probability of 'new': 0.003125\n",
            "Probability of 'information': 0.003125\n",
            "Probability of 'been': 0.003125\n",
            "Probability of 'processed': 0.003125\n",
            "Probability of 'real': 0.003125\n",
            "Probability of 'time': 0.003125\n",
            "Probability of 'addition': 0.003125\n",
            "Probability of 'come': 0.003125\n",
            "Probability of 'mix': 0.003125\n",
            "Probability of 'internal': 0.003125\n",
            "Probability of 'systems': 0.003125\n",
            "Probability of 'external': 0.003125\n",
            "Probability of 'sources': 0.003125\n",
            "Probability of 'at': 0.003125\n",
            "Probability of 'high': 0.003125\n",
            "Probability of 'level': 0.003125\n",
            "Probability of 'methodologies': 0.003125\n",
            "Probability of 'include': 0.003125\n",
            "Probability of 'exploratory': 0.00625\n",
            "Probability of 'analysis': 0.01875\n",
            "Probability of 'eda': 0.00625\n",
            "Probability of 'which': 0.00625\n",
            "Probability of 'aims': 0.003125\n",
            "Probability of 'find': 0.003125\n",
            "Probability of 'patterns': 0.003125\n",
            "Probability of 'relationships': 0.003125\n",
            "Probability of 'confirmatory': 0.003125\n",
            "Probability of 'cda': 0.00625\n",
            "Probability of 'applies': 0.003125\n",
            "Probability of 'statistical': 0.003125\n",
            "Probability of 'techniques': 0.003125\n",
            "Probability of 'determine': 0.003125\n",
            "Probability of 'whether': 0.003125\n",
            "Probability of 'hypotheses': 0.003125\n",
            "Probability of 'about': 0.003125\n",
            "Probability of 'set': 0.003125\n",
            "Probability of 'are': 0.003125\n",
            "Probability of 'true': 0.003125\n",
            "Probability of 'false': 0.003125\n",
            "Probability of 'often': 0.003125\n",
            "Probability of 'compared': 0.00625\n",
            "Probability of 'detective': 0.003125\n",
            "Probability of 'work': 0.00625\n",
            "Probability of 'akin': 0.003125\n",
            "Probability of 'judge': 0.003125\n",
            "Probability of 'jury': 0.003125\n",
            "Probability of 'during': 0.003125\n",
            "Probability of 'court': 0.003125\n",
            "Probability of 'trial': 0.003125\n",
            "Probability of 'distinction': 0.003125\n",
            "Probability of 'first': 0.003125\n",
            "Probability of 'drawn': 0.003125\n",
            "Probability of 'by': 0.003125\n",
            "Probability of 'statistician': 0.003125\n",
            "Probability of 'john': 0.003125\n",
            "Probability of 'w': 0.003125\n",
            "Probability of 'tukey': 0.003125\n",
            "Probability of 'his': 0.003125\n",
            "Probability of '1977': 0.003125\n",
            "Probability of 'book': 0.003125\n",
            "Probability of 'also': 0.003125\n",
            "Probability of 'be': 0.00625\n",
            "Probability of 'separated': 0.003125\n",
            "Probability of 'into': 0.003125\n",
            "Probability of 'quantitative': 0.003125\n",
            "Probability of 'qualitative': 0.00625\n",
            "Probability of 'former': 0.003125\n",
            "Probability of 'involves': 0.003125\n",
            "Probability of 'numerical': 0.00625\n",
            "Probability of 'quantifiable': 0.003125\n",
            "Probability of 'variables': 0.003125\n",
            "Probability of 'measured': 0.003125\n",
            "Probability of 'statistically': 0.003125\n",
            "Probability of 'approach': 0.003125\n",
            "Probability of 'interpretive': 0.003125\n",
            "Probability of 'focuses': 0.003125\n",
            "Probability of 'understanding': 0.003125\n",
            "Probability of 'content': 0.003125\n",
            "Probability of 'non': 0.003125\n",
            "Probability of 'like': 0.003125\n",
            "Probability of 'text': 0.003125\n",
            "Probability of 'images': 0.003125\n",
            "Probability of 'audio': 0.003125\n",
            "Probability of 'video': 0.003125\n",
            "Probability of 'including': 0.003125\n",
            "Probability of 'common': 0.003125\n",
            "Probability of 'phrases': 0.003125\n",
            "Probability of 'themes': 0.003125\n",
            "Probability of 'points': 0.003125\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "def tokenize(string):\n",
        "    return re.compile('\\w+').findall(string)\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "def word_freq(string): \n",
        "    text = tokenize(string.lower())\n",
        "    c = Counter(text)           \n",
        "    d = Counter(''.join(text))  \n",
        "    return (dict(c),dict(d))    \n",
        "\n",
        "data = file\n",
        "words, letters = word_freq(data) \n",
        "\n",
        "sumWords = sum(words.values())   \n",
        "\n",
        "for w in words:\n",
        "    print(\"Probability of '{}': {}\".format(w,words[w]/sumWords))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocess data\n",
        "file_nl_removed = \"\"\n",
        "for line in file:\n",
        "  line_nl_removed = line.replace(\"\\n\", \" \")      #removes newlines\n",
        "  file_nl_removed += line_nl_removed\n",
        "file_p = \"\".join([char for char in file_nl_removed if char not in string.punctuation])   #removes all special characters"
      ],
      "metadata": {
        "id": "qyJ3x0jA5rhF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prints the number of sentences\n",
        "sents = nltk.sent_tokenize(file_p)\n",
        "print(\"The number of sentences is\", len(sents)) \n",
        "\n",
        "#prints the number of tokens\n",
        "words = nltk.word_tokenize(file_p)\n",
        "print(\"The number of tokens is\", len(words)) \n",
        "\n",
        "#prints the average number of tokens per sentence\n",
        "average_tokens = round(len(words)/len(sents))\n",
        "print(\"The average number of tokens per sentence is\",\n",
        "average_tokens) \n",
        "\n",
        "#prints the number of unique tokens\n",
        "unique_tokens = set(words)\n",
        "print(\"The number of unique tokens are\", len(unique_tokens)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcPdlnFfXWFO",
        "outputId": "3f505aa2-dcd3-404f-a6d9-3d8ebbe9e349"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of sentences is 11\n",
            "The number of tokens is 325\n",
            "The average number of tokens per sentence is 30\n",
            "The number of unique tokens are 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "unigram=[]\n",
        "bigram=[]\n",
        "trigram=[]\n",
        "fourgram=[]\n",
        "tokenized_text = []\n",
        "for sentence in sents:\n",
        "    sentence = sentence.lower()\n",
        "    sequence = word_tokenize(sentence) \n",
        "    for word in sequence:\n",
        "        if (word =='.'):\n",
        "            sequence.remove(word) \n",
        "        else:\n",
        "            unigram.append(word)\n",
        "    tokenized_text.append(sequence) \n",
        "    #unigram, bigram, trigram, and fourgram models are created\n",
        "    bigram.extend(list(ngrams(sequence, 2)))  \n",
        "    trigram.extend(list(ngrams(sequence, 3)))\n",
        "    fourgram.extend(list(ngrams(sequence, 4)))\n",
        "def removal(x):     \n",
        "#removes ngrams containing only stopwords\n",
        "    y = []\n",
        "    for pair in x:\n",
        "        count = 0\n",
        "        for word in pair:\n",
        "            if word in stop_words:\n",
        "                count = count or 0\n",
        "            else:\n",
        "                count = count or 1\n",
        "        if (count==1):\n",
        "            y.append(pair)\n",
        "    return(y)\n",
        "bigram = removal(bigram)\n",
        "trigram = removal(trigram)             \n",
        "fourgram = removal(fourgram)\n",
        "freq_bi = nltk.FreqDist(bigram)\n",
        "freq_tri = nltk.FreqDist(trigram)\n",
        "freq_four = nltk.FreqDist(fourgram)\n",
        "\n",
        "print(\"Most common n-grams without stopword removal and without add-1 smoothing: \\n\")\n",
        "print (\"Most common bigrams: \", freq_bi.most_common(5))\n",
        "print (\"\\nMost common trigrams: \", freq_tri.most_common(5))\n",
        "print (\"\\nMost common fourgrams: \", freq_four.most_common(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRpg7PFjXi-G",
        "outputId": "0a279d6e-9b59-4792-afca-a78c77f19692"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most common n-grams without stopword removal and without add-1 smoothing: \n",
            "\n",
            "Most common bigrams:  [(('data', 'analytics'), 6), (('data', 'analysis'), 5), (('advanced', 'analytics'), 2), (('to', 'business'), 2), (('data', 'with'), 2)]\n",
            "\n",
            "Most common trigrams:  [(('exploratory', 'data', 'analysis'), 2), (('as', 'a', 'term'), 1), (('a', 'term', 'data'), 1), (('term', 'data', 'analytics'), 1), (('data', 'analytics', 'predominantly'), 1)]\n",
            "\n",
            "Most common fourgrams:  [(('as', 'a', 'term', 'data'), 1), (('a', 'term', 'data', 'analytics'), 1), (('term', 'data', 'analytics', 'predominantly'), 1), (('data', 'analytics', 'predominantly', 'refers'), 1), (('analytics', 'predominantly', 'refers', 'to'), 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Add 1-N Smooting**"
      ],
      "metadata": {
        "id": "Yl1DK1YBYPJs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used Maximum Likelihood Estimation (MLE) for training the parameters of an n-gram model. **The problem with MLE is that it assigns zero probability to unknown or unseen words**. This is because MLE uses a training corpus. If the word in the test set is not available in the training set, then the count of that particular word is zero and it leads to zero probability.\n",
        "\n",
        "To eliminate this zero probability, we can do smoothing. Smoothing involves taking some probability mass from the events seen in training and assigning it to unseen events. Add-1 smoothing or Laplace smoothing is a simple smoothing technique that adds 1 to the count of all n-grams in the training set before normalizing them into probabilities"
      ],
      "metadata": {
        "id": "WYF3cYSmYYAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Add-1 smoothing is performed here.\n",
        "            \n",
        "ngrams_all = {1:[], 2:[], 3:[], 4:[]}\n",
        "for i in range(4):\n",
        "    for each in tokenized_text:\n",
        "        for j in ngrams(each, i+1):\n",
        "            ngrams_all[i+1].append(j);\n",
        "ngrams_voc = {1:set([]), 2:set([]), 3:set([]), 4:set([])}\n",
        "for i in range(4):\n",
        "    for gram in ngrams_all[i+1]:\n",
        "        if gram not in ngrams_voc[i+1]:\n",
        "            ngrams_voc[i+1].add(gram)\n",
        "total_ngrams = {1:-1, 2:-1, 3:-1, 4:-1}\n",
        "total_voc = {1:-1, 2:-1, 3:-1, 4:-1}\n",
        "for i in range(4):\n",
        "    total_ngrams[i+1] = len(ngrams_all[i+1])\n",
        "    total_voc[i+1] = len(ngrams_voc[i+1])                       \n",
        "    \n",
        "ngrams_prob = {1:[], 2:[], 3:[], 4:[]}\n",
        "for i in range(4):\n",
        "    for ngram in ngrams_voc[i+1]:\n",
        "        tlist = [ngram]\n",
        "        tlist.append(ngrams_all[i+1].count(ngram))\n",
        "        ngrams_prob[i+1].append(tlist)\n",
        "    \n",
        "for i in range(4):\n",
        "    for ngram in ngrams_prob[i+1]:\n",
        "        ngram[-1] = (ngram[-1]+1)/(total_ngrams[i+1]+total_voc[i+1])    "
      ],
      "metadata": {
        "id": "MkJnsXegX_u-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prints top 10 unigram, bigram, trigram, fourgram after smoothing\n",
        "print(\"Most common n-grams without stopword removal and with add-1 smoothing: \\n\")\n",
        "for i in range(4):\n",
        "    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\n",
        "    \n",
        "print (\"Most common unigrams: \", str(ngrams_prob[1][:10]))\n",
        "print (\"\\nMost common bigrams: \", str(ngrams_prob[2][:10]))\n",
        "print (\"\\nMost common trigrams: \", str(ngrams_prob[3][:10]))\n",
        "print (\"\\nMost common fourgrams: \", str(ngrams_prob[4][:10]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRNecLL-YeTF",
        "outputId": "49fc1cf4-0864-4646-ab00-783cc8233f2b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most common n-grams without stopword removal and with add-1 smoothing: \n",
            "\n",
            "Most common unigrams:  [[('data',), 0.03740157480314961], [('the',), 0.023622047244094488], [('to',), 0.023622047244094488], [('a',), 0.021653543307086614], [('analytics',), 0.021653543307086614], [('of',), 0.021653543307086614], [('and',), 0.01968503937007874], [('analysis',), 0.013779527559055118], [('in',), 0.013779527559055118], [('can',), 0.011811023622047244]]\n",
            "\n",
            "Most common bigrams:  [[('data', 'analytics'), 0.011804384485666104], [('data', 'analysis'), 0.01011804384485666], [('exploratory', 'data'), 0.00505902192242833], [('to', 'business'), 0.00505902192242833], [('data', 'with'), 0.00505902192242833], [('advanced', 'analytics'), 0.00505902192242833], [('with', 'the'), 0.00505902192242833], [('as', 'a'), 0.00505902192242833], [('service', 'efforts'), 0.003372681281618887], [('a', 'competitive'), 0.003372681281618887]]\n",
            "\n",
            "Most common trigrams:  [[('exploratory', 'data', 'analysis'), 0.005128205128205128], [('distinction', 'first', 'drawn'), 0.003418803418803419], [('analytics', 'has', 'a'), 0.003418803418803419], [('to', 'analyzing', 'data'), 0.003418803418803419], [('analyzing', 'data', 'with'), 0.003418803418803419], [('of', 'nonnumerical', 'data'), 0.003418803418803419], [('of', 'advanced', 'analytics'), 0.003418803418803419], [('themes', 'and', 'points'), 0.003418803418803419], [('as', 'a', 'separate'), 0.003418803418803419], [('umbrella', 'term', 'for'), 0.003418803418803419]]\n",
            "\n",
            "Most common fourgrams:  [[('of', 'internal', 'systems', 'and'), 0.0035460992907801418], [('data', 'analysis', 'cda', 'which'), 0.0035460992907801418], [('quickly', 'to', 'emerging', 'market'), 0.0035460992907801418], [('into', 'quantitative', 'data', 'analysis'), 0.0035460992907801418], [('eda', 'is', 'often', 'compared'), 0.0035460992907801418], [('first', 'drawn', 'by', 'statistician'), 0.0035460992907801418], [('oriented', 'to', 'business', 'uses'), 0.0035460992907801418], [('analyzing', 'data', 'with', 'the'), 0.0035460992907801418], [('cases', 'people', 'use', 'data'), 0.0035460992907801418], [('of', 'boosting', 'business', 'performance'), 0.0035460992907801418]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8mxafpUFYi09"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}